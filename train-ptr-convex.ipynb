{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointer networks basic implementation\n",
    "\n",
    "## Tasks\n",
    "Pick \"convex hull\"\n",
    "* [x] Generate the dataset\n",
    "* [x] Evaluation metric\n",
    "* [x] Implement the model\n",
    "* [ ] Reproduce the results from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points_and_hull(points, hull_indices, c='r'):\n",
    "    hull_indices = np.hstack([hull_indices, [hull_indices[0]]])\n",
    "\n",
    "    points_hull = points[hull_indices]\n",
    "    points_hull = points_hull[points_hull[:, 0] != -1]\n",
    "    \n",
    "    points = points[points[:, 0] != -1]\n",
    "\n",
    "    print('{} points, {} in the hull'.format(points.shape[0], points_hull.shape[0]))\n",
    "    plt.scatter(points[:, 0], points[:, 1])\n",
    "    plt.plot(points_hull[:, 0], points_hull[:, 1], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = collections.namedtuple('Params', [\n",
    "    'gpu_device',\n",
    "    'batch_size', 'embedding_size', 'hiddens', 'n_lstms', 'dropout', 'bidir',\n",
    "    'lr', 'n_epochs',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params(\n",
    "    gpu_device=2,\n",
    "    \n",
    "    # Data\n",
    "    batch_size=256,\n",
    "    \n",
    "    # Training params\n",
    "    lr=1e-4,\n",
    "    n_epochs=50,\n",
    "    \n",
    "    # Model params # FIXME: NOT USED RIGHT NOW!\n",
    "    embedding_size=128,\n",
    "    hiddens=512,\n",
    "    n_lstms=2,\n",
    "    dropout=0,\n",
    "    bidir=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = params.gpu_device >= 0 and torch.cuda.is_available()\n",
    "DEVICE = 'cpu'\n",
    "if USE_CUDA:\n",
    "    DEVICE = 'cuda:{}'.format(params.gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ConvexHullDataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### For convex hull\n",
    "# The data was generated using convex_hull_generator.py\n",
    "data = np.load('data/convex_hull_5.npz')\n",
    "# data = np.load('data/convex_hull.npz')\n",
    "\n",
    "data_train, data_val, data_test = data['arr_0']\n",
    "\n",
    "data_train = np.array(data_train)\n",
    "data_val = np.array(data_val)\n",
    "data_test = np.array(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ConvexHullDataset(data_train, append_eol=True)\n",
    "dataset_val = ConvexHullDataset(data_val, append_eol=True)\n",
    "dataset_test = ConvexHullDataset(data_test, append_eol=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, batch_size=params.batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=params.batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=params.batch_size, shuffle=False, num_workers=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = iter(dataloader_train)\n",
    "for ix in range(5):\n",
    "    batch = next(d)\n",
    "\n",
    "    plt.figure()\n",
    "    points = batch['sequence'][0].data.numpy()\n",
    "    inds_hull = batch['pointers'][0].data.numpy().ravel()\n",
    "    inds_hull = inds_hull[: batch['pointer_lens'][0]]\n",
    "    plot_points_and_hull(points, inds_hull)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointer_net import PointerNet, Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointerNet()\n",
    "\n",
    "# SANITY RUN THE MODEL\n",
    "batch = next(iter(dataloader_val))\n",
    "\n",
    "seq = batch['sequence']\n",
    "seq_lens = batch['sequence_lens']\n",
    "\n",
    "target_pointers = batch['pointers']\n",
    "pointer_lens = batch['pointer_lens']\n",
    "\n",
    "pointers = model(seq, seq_lens, max_output_len=target_pointers.shape[1])\n",
    "pointers.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CUDA:\n",
    "    model.cuda(device=params.gpu_device)\n",
    "#     cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer / loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=-100).to(DEVICE)  # -1 for the padded elements\n",
    "model_optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=params.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_str = 'ptr-convex-hull-batched-5-eol-1.00'\n",
    "\n",
    "# logging\n",
    "weights_folder = \"/opt/weights/{}\".format(model_str)\n",
    "log_folder =  '../tensorboard-logs/{}'.format(model_str)\n",
    "writer = SummaryWriter(log_folder) # writing log to tensorboard\n",
    "print('logging to: {}'.format(weights_folder))\n",
    "\n",
    "os.makedirs(weights_folder)  # MEANT TO FAIL IF IT ALREADY EXISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_every = 10000\n",
    "val_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_eval(model, batch, loss_func):\n",
    "    seq = Variable(batch['sequence'])\n",
    "    seq_lens, target_pointer_lens = batch['sequence_lens'], batch['pointer_lens']\n",
    "    target_pointers = Variable(batch['pointers'])  # FIXME: Must append an EOS token, subtract 1 to make 0-based\n",
    "\n",
    "    if USE_CUDA:\n",
    "        seq = seq.cuda(params.gpu_device)\n",
    "        target_pointers = target_pointers.cuda(params.gpu_device)\n",
    "\n",
    "    # generate as many outputs as in the target sequence\n",
    "    n_outputs = max(target_pointer_lens)\n",
    "    pointers = model(seq, seq_lens, max_output_len=n_outputs)  # FIXME: because we don't have an EOS token. Also, makes sense during traing\n",
    "    assert n_outputs == pointers.shape[1]\n",
    "\n",
    "    n_classes = pointers.shape[-1]\n",
    "    loss = loss_func(pointers.contiguous().view(-1, n_classes), target_pointers.contiguous().view(-1))\n",
    "    return pointers, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while epoch < 5000:  # params.n_epochs:\n",
    "    for i_batch, train_batch in enumerate(dataloader_train):\n",
    "        iter_cntr = epoch * len(dataloader_train) + i_batch  # The overall iteration number across epochs\n",
    "\n",
    "        # Forward\n",
    "        pointers, train_loss = predict_and_eval(model, train_batch, loss_func)\n",
    "\n",
    "        # Backprop\n",
    "        model_optim.zero_grad()\n",
    "        train_loss.backward()\n",
    "        model_optim.step()\n",
    "\n",
    "        writer.add_scalar('train.loss', train_loss.data.cpu().numpy(), iter_cntr)\n",
    "        \n",
    "        # Save\n",
    "        if i_batch % save_every == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(weights_folder, '{}_{}.pt'.format(epoch, i_batch)))\n",
    "        \n",
    "        # Validation\n",
    "        if i_batch % val_every == 0:\n",
    "            plt.figure(figsize=(5, 5))\n",
    "\n",
    "            total_val_loss = 0\n",
    "            for jx, val_batch in enumerate(dataloader_val):\n",
    "                if jx == 10:\n",
    "                    break\n",
    "                pointers, val_loss = predict_and_eval(model, val_batch, loss_func)\n",
    "                total_val_loss += val_loss.data.cpu().numpy()\n",
    "\n",
    "                # plot few\n",
    "                if jx < 4:\n",
    "                    plt.subplot(2, 2, jx+1)\n",
    "                    pred_indices = pointers.argmax(dim=-1).data.cpu().numpy()\n",
    "                    \n",
    "                    target_indices = val_batch['pointers'][0].data.cpu().numpy()\n",
    "                    assert len(target_indices) == pred_indices.shape[1]\n",
    "                    print('Targets: {}, Preds: {}'.format(target_indices.flatten(), pred_indices[0].flatten()))\n",
    "                    seq_lens = val_batch['sequence_lens']\n",
    "                    pointer_lens = val_batch['pointer_lens']\n",
    "                    points = val_batch['sequence'][0].data.cpu().numpy()[: seq_lens[0]]\n",
    "                    plot_points_and_hull(points, pred_indices[0].flatten()[: pointer_lens[0]], c='b')\n",
    "                    plot_points_and_hull(points, target_indices[: pointer_lens[0]].flatten(), c='r--')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            writer.add_scalar('val.loss', total_val_loss / 10, iter_cntr)\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
